{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing FunkSVD\n",
    "\n",
    "In this notebook we will take a look at writing our own function that performs FunkSVD, which will follow the steps you saw in the previous video.  If you find that you aren't ready to tackle this task on your own, feel free to skip to the following video where you can watch as I walk through the steps.\n",
    "\n",
    "To test our algorithm, we will run it on the subset of the data you worked with earlier.  Run the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randomwalk10/anaconda3/envs/aind/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/randomwalk10/anaconda3/envs/aind/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10. 10. 10. 10.]\n",
      " [10.  4.  9. 10.]\n",
      " [ 8.  9. 10.  5.]\n",
      " [ 9.  8. 10. 10.]\n",
      " [10.  5.  9.  9.]\n",
      " [ 6.  4. 10.  6.]\n",
      " [ 9.  8. 10.  9.]\n",
      " [10.  5.  9.  8.]\n",
      " [ 7.  8. 10.  8.]\n",
      " [ 9.  5.  9.  7.]\n",
      " [ 9.  8. 10.  8.]\n",
      " [ 9. 10. 10.  9.]\n",
      " [10.  9. 10.  8.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [10.  8. 10. 10.]\n",
      " [ 9.  9. 10. 10.]\n",
      " [ 9.  8.  8.  8.]\n",
      " [10.  8.  1. 10.]\n",
      " [ 5.  6. 10. 10.]\n",
      " [ 8.  7. 10.  7.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "# Create user-by-item matrix\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "# Create data subset\n",
    "user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "ratings_mat = np.matrix(user_movie_subset)\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` You will use the **user_movie_subset** matrix to show that your FunkSVD algorithm will converge.  In the below cell, use the comments and document string to assist you as you complete writing your own function to complete FunkSVD.  You may also want to try to complete the function on your own without the assistance of comments.  You may feel free to remove and add to the function in any way that gets you a working solution! \n",
    "\n",
    "**Notice:** There isn't a sigma matrix in this version of the matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movie_id</th>\n",
       "      <th>73486</th>\n",
       "      <th>75314</th>\n",
       "      <th>68646</th>\n",
       "      <th>99685</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11639</th>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13006</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14076</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14725</th>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23548</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24760</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28713</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30685</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34110</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34430</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35150</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43294</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46849</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50556</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51382</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51410</th>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movie_id  73486  75314  68646  99685\n",
       "user_id                             \n",
       "265        10.0   10.0   10.0   10.0\n",
       "1023       10.0    4.0    9.0   10.0\n",
       "1683        8.0    9.0   10.0    5.0\n",
       "6571        9.0    8.0   10.0   10.0\n",
       "11639      10.0    5.0    9.0    9.0\n",
       "13006       6.0    4.0   10.0    6.0\n",
       "14076       9.0    8.0   10.0    9.0\n",
       "14725      10.0    5.0    9.0    8.0\n",
       "23548       7.0    8.0   10.0    8.0\n",
       "24760       9.0    5.0    9.0    7.0\n",
       "28713       9.0    8.0   10.0    8.0\n",
       "30685       9.0   10.0   10.0    9.0\n",
       "34110      10.0    9.0   10.0    8.0\n",
       "34430       5.0    8.0    5.0    8.0\n",
       "35150      10.0    8.0   10.0   10.0\n",
       "43294       9.0    9.0   10.0   10.0\n",
       "46849       9.0    8.0    8.0    8.0\n",
       "50556      10.0    8.0    1.0   10.0\n",
       "51382       5.0    6.0   10.0   10.0\n",
       "51410       8.0    7.0   10.0    7.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movie_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0] # number of rows in the matrix\n",
    "    n_movies = ratings_mat.shape[1] # number of movies in the matrix\n",
    "    num_ratings = n_users*n_movies-np.sum(np.isnan(ratings_mat)) # total number of ratings in the matrix\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    # helpful link: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html\n",
    "    user_mat = np.random.rand(n_users, latent_features) # user matrix filled with random values of shape user x latent \n",
    "    movie_mat = np.random.rand(latent_features, n_movies) # movie matrix filled with random values of shape latent x movies\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # header for running results\n",
    "    print(\"Optimization Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for i in range(iters):\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for r in range(n_users):\n",
    "            for c in range(n_movies):\n",
    "                # if the rating exists\n",
    "                if not np.isnan(ratings_mat.item(r, c)):    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    err = ratings_mat.item(r,c) - np.dot(user_mat[r, :], movie_mat[:, c])\n",
    "                    # Keep track of the total sum of squared errors for the matrix\n",
    "                    sse_accum += err**2\n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    old_user_row = user_mat[r, :]\n",
    "                    user_mat[r, :] += 2*err*learning_rate*movie_mat[:, c]\n",
    "                    movie_mat[:, c] += 2*err*learning_rate*old_user_row\n",
    "        # print results for iteration\n",
    "        print(\"#%d sse_accum is %f and delta is %f\" % (i+1, sse_accum, sse_accum-old_sse))\n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Try out your function on the **user_movie_subset** dataset.  First try 4 latent features, a learning rate of 0.005, and 10 iterations.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Statistics\n",
      "Iterations | Mean Squared Error \n",
      "#1 sse_accum is 3189.714530 and delta is 3189.714530\n",
      "#2 sse_accum is 1003.442285 and delta is -2186.272244\n",
      "#3 sse_accum is 297.053296 and delta is -706.388989\n",
      "#4 sse_accum is 224.981003 and delta is -72.072293\n",
      "#5 sse_accum is 216.828876 and delta is -8.152127\n",
      "#6 sse_accum is 213.248055 and delta is -3.580821\n",
      "#7 sse_accum is 210.099690 and delta is -3.148365\n",
      "#8 sse_accum is 206.703478 and delta is -3.396212\n",
      "#9 sse_accum is 202.901036 and delta is -3.802442\n",
      "#10 sse_accum is 198.640097 and delta is -4.260938\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=10)# use your function with 4 latent features, lr of 0.005 and 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.0844403   8.78763149 10.78282824 10.04418275]\n",
      " [ 8.6112317   7.38929084  9.36531398  8.60249284]\n",
      " [ 7.82166888  6.75007573  8.25812776  7.86864034]\n",
      " [ 9.54937019  8.32999045 10.03396714  9.5549262 ]\n",
      " [ 8.3948008   7.11927623  9.46954688  8.36444964]\n",
      " [ 6.68045882  5.61149036  7.92460052  6.5738904 ]\n",
      " [ 9.25546842  8.10878076  9.50392909  9.28392843]\n",
      " [ 8.18479475  7.09378536  8.66282812  8.21013326]\n",
      " [ 8.50586866  7.41066219  8.85847424  8.52718736]\n",
      " [ 7.74753806  6.84376602  7.83983189  7.76552392]\n",
      " [ 8.9406097   7.87959103  9.10617482  8.96538947]\n",
      " [ 9.67534613  8.65674278  9.78615671  9.62612606]\n",
      " [ 9.36068342  8.27656924  9.4015917   9.39882102]\n",
      " [ 6.81059628  6.13410897  6.27542265  6.91417463]\n",
      " [ 9.62045883  8.25503883 10.67105261  9.5676846 ]\n",
      " [ 9.7588335   8.56088416 10.1854373   9.74753842]\n",
      " [ 8.37793929  7.4602953   8.39539936  8.38395904]\n",
      " [ 7.24671179  6.52635523  6.79857939  7.32797494]\n",
      " [ 8.32373764  7.20482674  9.08638371  8.26913081]\n",
      " [ 8.07145811  6.90530496  8.74374919  8.09508013]]\n",
      "[[10. 10. 10. 10.]\n",
      " [10.  4.  9. 10.]\n",
      " [ 8.  9. 10.  5.]\n",
      " [ 9.  8. 10. 10.]\n",
      " [10.  5.  9.  9.]\n",
      " [ 6.  4. 10.  6.]\n",
      " [ 9.  8. 10.  9.]\n",
      " [10.  5.  9.  8.]\n",
      " [ 7.  8. 10.  8.]\n",
      " [ 9.  5.  9.  7.]\n",
      " [ 9.  8. 10.  8.]\n",
      " [ 9. 10. 10.  9.]\n",
      " [10.  9. 10.  8.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [10.  8. 10. 10.]\n",
      " [ 9.  9. 10. 10.]\n",
      " [ 9.  8.  8.  8.]\n",
      " [10.  8.  1. 10.]\n",
      " [ 5.  6. 10. 10.]\n",
      " [ 8.  7. 10.  7.]]\n"
     ]
    }
   ],
   "source": [
    "#Compare the predicted and actual results\n",
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your summary here.**\n",
    "\n",
    "The result of FunkSVD is an approximation of the original subset of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Let's try out the function again on the **user_movie_subset** dataset.  This time we will again use 4 latent features and a learning rate of 0.005.  However, let's bump up the number of iterations to 250.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?  What do you notice about your error at the end of the 250 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Statistics\n",
      "Iterations | Mean Squared Error \n",
      "#1 sse_accum is 3689.893896 and delta is 3689.893896\n",
      "#2 sse_accum is 1351.478882 and delta is -2338.415014\n",
      "#3 sse_accum is 350.219625 and delta is -1001.259257\n",
      "#4 sse_accum is 219.829410 and delta is -130.390215\n",
      "#5 sse_accum is 207.351339 and delta is -12.478071\n",
      "#6 sse_accum is 203.121877 and delta is -4.229461\n",
      "#7 sse_accum is 199.424876 and delta is -3.697001\n",
      "#8 sse_accum is 195.250005 and delta is -4.174872\n",
      "#9 sse_accum is 190.406456 and delta is -4.843549\n",
      "#10 sse_accum is 184.854106 and delta is -5.552350\n",
      "#11 sse_accum is 178.590008 and delta is -6.264098\n",
      "#12 sse_accum is 171.640163 and delta is -6.949845\n",
      "#13 sse_accum is 164.067603 and delta is -7.572561\n",
      "#14 sse_accum is 155.978435 and delta is -8.089168\n",
      "#15 sse_accum is 147.521332 and delta is -8.457103\n",
      "#16 sse_accum is 138.878801 and delta is -8.642530\n",
      "#17 sse_accum is 130.250419 and delta is -8.628382\n",
      "#18 sse_accum is 121.830424 and delta is -8.419996\n",
      "#19 sse_accum is 113.784245 and delta is -8.046179\n",
      "#20 sse_accum is 106.229517 and delta is -7.554728\n",
      "#21 sse_accum is 99.226158 and delta is -7.003359\n",
      "#22 sse_accum is 92.777415 and delta is -6.448743\n",
      "#23 sse_accum is 86.840499 and delta is -5.936917\n",
      "#24 sse_accum is 81.342954 and delta is -5.497545\n",
      "#25 sse_accum is 76.200196 and delta is -5.142758\n",
      "#26 sse_accum is 71.330474 and delta is -4.869723\n",
      "#27 sse_accum is 66.665232 and delta is -4.665241\n",
      "#28 sse_accum is 62.154524 and delta is -4.510708\n",
      "#29 sse_accum is 57.768207 and delta is -4.386318\n",
      "#30 sse_accum is 53.494162 and delta is -4.274044\n",
      "#31 sse_accum is 49.334775 and delta is -4.159387\n",
      "#32 sse_accum is 45.302672 and delta is -4.032104\n",
      "#33 sse_accum is 41.416451 and delta is -3.886221\n",
      "#34 sse_accum is 37.696889 and delta is -3.719562\n",
      "#35 sse_accum is 34.163885 and delta is -3.533004\n",
      "#36 sse_accum is 30.834271 and delta is -3.329614\n",
      "#37 sse_accum is 27.720481 and delta is -3.113789\n",
      "#38 sse_accum is 24.829985 and delta is -2.890496\n",
      "#39 sse_accum is 22.165324 and delta is -2.664662\n",
      "#40 sse_accum is 19.724575 and delta is -2.440749\n",
      "#41 sse_accum is 17.502061 and delta is -2.222513\n",
      "#42 sse_accum is 15.489157 and delta is -2.012904\n",
      "#43 sse_accum is 13.675075 and delta is -1.814083\n",
      "#44 sse_accum is 12.047572 and delta is -1.627503\n",
      "#45 sse_accum is 10.593546 and delta is -1.454026\n",
      "#46 sse_accum is 9.299507 and delta is -1.294039\n",
      "#47 sse_accum is 8.151944 and delta is -1.147564\n",
      "#48 sse_accum is 7.137601 and delta is -1.014343\n",
      "#49 sse_accum is 6.243685 and delta is -0.893915\n",
      "#50 sse_accum is 5.458016 and delta is -0.785670\n",
      "#51 sse_accum is 4.769127 and delta is -0.688888\n",
      "#52 sse_accum is 4.166343 and delta is -0.602784\n",
      "#53 sse_accum is 3.639815 and delta is -0.526528\n",
      "#54 sse_accum is 3.180540 and delta is -0.459275\n",
      "#55 sse_accum is 2.780355 and delta is -0.400185\n",
      "#56 sse_accum is 2.431918 and delta is -0.348438\n",
      "#57 sse_accum is 2.128666 and delta is -0.303252\n",
      "#58 sse_accum is 1.864777 and delta is -0.263889\n",
      "#59 sse_accum is 1.635110 and delta is -0.229667\n",
      "#60 sse_accum is 1.435153 and delta is -0.199957\n",
      "#61 sse_accum is 1.260959 and delta is -0.174193\n",
      "#62 sse_accum is 1.109093 and delta is -0.151866\n",
      "#63 sse_accum is 0.976570 and delta is -0.132523\n",
      "#64 sse_accum is 0.860806 and delta is -0.115764\n",
      "#65 sse_accum is 0.759568 and delta is -0.101239\n",
      "#66 sse_accum is 0.670925 and delta is -0.088642\n",
      "#67 sse_accum is 0.593216 and delta is -0.077709\n",
      "#68 sse_accum is 0.525005 and delta is -0.068210\n",
      "#69 sse_accum is 0.465057 and delta is -0.059948\n",
      "#70 sse_accum is 0.412306 and delta is -0.052751\n",
      "#71 sse_accum is 0.365831 and delta is -0.046475\n",
      "#72 sse_accum is 0.324838 and delta is -0.040993\n",
      "#73 sse_accum is 0.288640 and delta is -0.036199\n",
      "#74 sse_accum is 0.256641 and delta is -0.031999\n",
      "#75 sse_accum is 0.228325 and delta is -0.028315\n",
      "#76 sse_accum is 0.203246 and delta is -0.025079\n",
      "#77 sse_accum is 0.181013 and delta is -0.022233\n",
      "#78 sse_accum is 0.161287 and delta is -0.019726\n",
      "#79 sse_accum is 0.143771 and delta is -0.017516\n",
      "#80 sse_accum is 0.128207 and delta is -0.015564\n",
      "#81 sse_accum is 0.114369 and delta is -0.013839\n",
      "#82 sse_accum is 0.102056 and delta is -0.012312\n",
      "#83 sse_accum is 0.091096 and delta is -0.010960\n",
      "#84 sse_accum is 0.081335 and delta is -0.009762\n",
      "#85 sse_accum is 0.072636 and delta is -0.008698\n",
      "#86 sse_accum is 0.064882 and delta is -0.007754\n",
      "#87 sse_accum is 0.057968 and delta is -0.006915\n",
      "#88 sse_accum is 0.051799 and delta is -0.006169\n",
      "#89 sse_accum is 0.046294 and delta is -0.005505\n",
      "#90 sse_accum is 0.041380 and delta is -0.004914\n",
      "#91 sse_accum is 0.036992 and delta is -0.004388\n",
      "#92 sse_accum is 0.033074 and delta is -0.003919\n",
      "#93 sse_accum is 0.029573 and delta is -0.003500\n",
      "#94 sse_accum is 0.026446 and delta is -0.003127\n",
      "#95 sse_accum is 0.023651 and delta is -0.002795\n",
      "#96 sse_accum is 0.021153 and delta is -0.002498\n",
      "#97 sse_accum is 0.018921 and delta is -0.002233\n",
      "#98 sse_accum is 0.016925 and delta is -0.001996\n",
      "#99 sse_accum is 0.015140 and delta is -0.001785\n",
      "#100 sse_accum is 0.013544 and delta is -0.001596\n",
      "#101 sse_accum is 0.012117 and delta is -0.001427\n",
      "#102 sse_accum is 0.010840 and delta is -0.001276\n",
      "#103 sse_accum is 0.009699 and delta is -0.001142\n",
      "#104 sse_accum is 0.008677 and delta is -0.001021\n",
      "#105 sse_accum is 0.007764 and delta is -0.000913\n",
      "#106 sse_accum is 0.006947 and delta is -0.000817\n",
      "#107 sse_accum is 0.006216 and delta is -0.000731\n",
      "#108 sse_accum is 0.005562 and delta is -0.000654\n",
      "#109 sse_accum is 0.004977 and delta is -0.000585\n",
      "#110 sse_accum is 0.004453 and delta is -0.000524\n",
      "#111 sse_accum is 0.003985 and delta is -0.000468\n",
      "#112 sse_accum is 0.003565 and delta is -0.000419\n",
      "#113 sse_accum is 0.003190 and delta is -0.000375\n",
      "#114 sse_accum is 0.002855 and delta is -0.000336\n",
      "#115 sse_accum is 0.002554 and delta is -0.000300\n",
      "#116 sse_accum is 0.002286 and delta is -0.000269\n",
      "#117 sse_accum is 0.002045 and delta is -0.000240\n",
      "#118 sse_accum is 0.001830 and delta is -0.000215\n",
      "#119 sse_accum is 0.001638 and delta is -0.000193\n",
      "#120 sse_accum is 0.001465 and delta is -0.000172\n",
      "#121 sse_accum is 0.001311 and delta is -0.000154\n",
      "#122 sse_accum is 0.001173 and delta is -0.000138\n",
      "#123 sse_accum is 0.001050 and delta is -0.000123\n",
      "#124 sse_accum is 0.000939 and delta is -0.000110\n",
      "#125 sse_accum is 0.000841 and delta is -0.000099\n",
      "#126 sse_accum is 0.000752 and delta is -0.000088\n",
      "#127 sse_accum is 0.000673 and delta is -0.000079\n",
      "#128 sse_accum is 0.000602 and delta is -0.000071\n",
      "#129 sse_accum is 0.000539 and delta is -0.000063\n",
      "#130 sse_accum is 0.000482 and delta is -0.000057\n",
      "#131 sse_accum is 0.000431 and delta is -0.000051\n",
      "#132 sse_accum is 0.000386 and delta is -0.000045\n",
      "#133 sse_accum is 0.000345 and delta is -0.000041\n",
      "#134 sse_accum is 0.000309 and delta is -0.000036\n",
      "#135 sse_accum is 0.000276 and delta is -0.000033\n",
      "#136 sse_accum is 0.000247 and delta is -0.000029\n",
      "#137 sse_accum is 0.000221 and delta is -0.000026\n",
      "#138 sse_accum is 0.000198 and delta is -0.000023\n",
      "#139 sse_accum is 0.000177 and delta is -0.000021\n",
      "#140 sse_accum is 0.000158 and delta is -0.000019\n",
      "#141 sse_accum is 0.000142 and delta is -0.000017\n",
      "#142 sse_accum is 0.000127 and delta is -0.000015\n",
      "#143 sse_accum is 0.000113 and delta is -0.000013\n",
      "#144 sse_accum is 0.000102 and delta is -0.000012\n",
      "#145 sse_accum is 0.000091 and delta is -0.000011\n",
      "#146 sse_accum is 0.000081 and delta is -0.000010\n",
      "#147 sse_accum is 0.000073 and delta is -0.000009\n",
      "#148 sse_accum is 0.000065 and delta is -0.000008\n",
      "#149 sse_accum is 0.000058 and delta is -0.000007\n",
      "#150 sse_accum is 0.000052 and delta is -0.000006\n",
      "#151 sse_accum is 0.000047 and delta is -0.000005\n",
      "#152 sse_accum is 0.000042 and delta is -0.000005\n",
      "#153 sse_accum is 0.000037 and delta is -0.000004\n",
      "#154 sse_accum is 0.000033 and delta is -0.000004\n",
      "#155 sse_accum is 0.000030 and delta is -0.000004\n",
      "#156 sse_accum is 0.000027 and delta is -0.000003\n",
      "#157 sse_accum is 0.000024 and delta is -0.000003\n",
      "#158 sse_accum is 0.000021 and delta is -0.000003\n",
      "#159 sse_accum is 0.000019 and delta is -0.000002\n",
      "#160 sse_accum is 0.000017 and delta is -0.000002\n",
      "#161 sse_accum is 0.000015 and delta is -0.000002\n",
      "#162 sse_accum is 0.000014 and delta is -0.000002\n",
      "#163 sse_accum is 0.000012 and delta is -0.000001\n",
      "#164 sse_accum is 0.000011 and delta is -0.000001\n",
      "#165 sse_accum is 0.000010 and delta is -0.000001\n",
      "#166 sse_accum is 0.000009 and delta is -0.000001\n",
      "#167 sse_accum is 0.000008 and delta is -0.000001\n",
      "#168 sse_accum is 0.000007 and delta is -0.000001\n",
      "#169 sse_accum is 0.000006 and delta is -0.000001\n",
      "#170 sse_accum is 0.000006 and delta is -0.000001\n",
      "#171 sse_accum is 0.000005 and delta is -0.000001\n",
      "#172 sse_accum is 0.000004 and delta is -0.000001\n",
      "#173 sse_accum is 0.000004 and delta is -0.000000\n",
      "#174 sse_accum is 0.000004 and delta is -0.000000\n",
      "#175 sse_accum is 0.000003 and delta is -0.000000\n",
      "#176 sse_accum is 0.000003 and delta is -0.000000\n",
      "#177 sse_accum is 0.000003 and delta is -0.000000\n",
      "#178 sse_accum is 0.000002 and delta is -0.000000\n",
      "#179 sse_accum is 0.000002 and delta is -0.000000\n",
      "#180 sse_accum is 0.000002 and delta is -0.000000\n",
      "#181 sse_accum is 0.000002 and delta is -0.000000\n",
      "#182 sse_accum is 0.000001 and delta is -0.000000\n",
      "#183 sse_accum is 0.000001 and delta is -0.000000\n",
      "#184 sse_accum is 0.000001 and delta is -0.000000\n",
      "#185 sse_accum is 0.000001 and delta is -0.000000\n",
      "#186 sse_accum is 0.000001 and delta is -0.000000\n",
      "#187 sse_accum is 0.000001 and delta is -0.000000\n",
      "#188 sse_accum is 0.000001 and delta is -0.000000\n",
      "#189 sse_accum is 0.000001 and delta is -0.000000\n",
      "#190 sse_accum is 0.000001 and delta is -0.000000\n",
      "#191 sse_accum is 0.000001 and delta is -0.000000\n",
      "#192 sse_accum is 0.000000 and delta is -0.000000\n",
      "#193 sse_accum is 0.000000 and delta is -0.000000\n",
      "#194 sse_accum is 0.000000 and delta is -0.000000\n",
      "#195 sse_accum is 0.000000 and delta is -0.000000\n",
      "#196 sse_accum is 0.000000 and delta is -0.000000\n",
      "#197 sse_accum is 0.000000 and delta is -0.000000\n",
      "#198 sse_accum is 0.000000 and delta is -0.000000\n",
      "#199 sse_accum is 0.000000 and delta is -0.000000\n",
      "#200 sse_accum is 0.000000 and delta is -0.000000\n",
      "#201 sse_accum is 0.000000 and delta is -0.000000\n",
      "#202 sse_accum is 0.000000 and delta is -0.000000\n",
      "#203 sse_accum is 0.000000 and delta is -0.000000\n",
      "#204 sse_accum is 0.000000 and delta is -0.000000\n",
      "#205 sse_accum is 0.000000 and delta is -0.000000\n",
      "#206 sse_accum is 0.000000 and delta is -0.000000\n",
      "#207 sse_accum is 0.000000 and delta is -0.000000\n",
      "#208 sse_accum is 0.000000 and delta is -0.000000\n",
      "#209 sse_accum is 0.000000 and delta is -0.000000\n",
      "#210 sse_accum is 0.000000 and delta is -0.000000\n",
      "#211 sse_accum is 0.000000 and delta is -0.000000\n",
      "#212 sse_accum is 0.000000 and delta is -0.000000\n",
      "#213 sse_accum is 0.000000 and delta is -0.000000\n",
      "#214 sse_accum is 0.000000 and delta is -0.000000\n",
      "#215 sse_accum is 0.000000 and delta is -0.000000\n",
      "#216 sse_accum is 0.000000 and delta is -0.000000\n",
      "#217 sse_accum is 0.000000 and delta is -0.000000\n",
      "#218 sse_accum is 0.000000 and delta is -0.000000\n",
      "#219 sse_accum is 0.000000 and delta is -0.000000\n",
      "#220 sse_accum is 0.000000 and delta is -0.000000\n",
      "#221 sse_accum is 0.000000 and delta is -0.000000\n",
      "#222 sse_accum is 0.000000 and delta is -0.000000\n",
      "#223 sse_accum is 0.000000 and delta is -0.000000\n",
      "#224 sse_accum is 0.000000 and delta is -0.000000\n",
      "#225 sse_accum is 0.000000 and delta is -0.000000\n",
      "#226 sse_accum is 0.000000 and delta is -0.000000\n",
      "#227 sse_accum is 0.000000 and delta is -0.000000\n",
      "#228 sse_accum is 0.000000 and delta is -0.000000\n",
      "#229 sse_accum is 0.000000 and delta is -0.000000\n",
      "#230 sse_accum is 0.000000 and delta is -0.000000\n",
      "#231 sse_accum is 0.000000 and delta is -0.000000\n",
      "#232 sse_accum is 0.000000 and delta is -0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#233 sse_accum is 0.000000 and delta is -0.000000\n",
      "#234 sse_accum is 0.000000 and delta is -0.000000\n",
      "#235 sse_accum is 0.000000 and delta is -0.000000\n",
      "#236 sse_accum is 0.000000 and delta is -0.000000\n",
      "#237 sse_accum is 0.000000 and delta is -0.000000\n",
      "#238 sse_accum is 0.000000 and delta is -0.000000\n",
      "#239 sse_accum is 0.000000 and delta is -0.000000\n",
      "#240 sse_accum is 0.000000 and delta is -0.000000\n",
      "#241 sse_accum is 0.000000 and delta is -0.000000\n",
      "#242 sse_accum is 0.000000 and delta is -0.000000\n",
      "#243 sse_accum is 0.000000 and delta is -0.000000\n",
      "#244 sse_accum is 0.000000 and delta is -0.000000\n",
      "#245 sse_accum is 0.000000 and delta is -0.000000\n",
      "#246 sse_accum is 0.000000 and delta is -0.000000\n",
      "#247 sse_accum is 0.000000 and delta is -0.000000\n",
      "#248 sse_accum is 0.000000 and delta is -0.000000\n",
      "#249 sse_accum is 0.000000 and delta is -0.000000\n",
      "#250 sse_accum is 0.000000 and delta is -0.000000\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=250) #use your function with 4 latent features, lr of 0.005 and 250 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.99999488  9.99999703  9.99999882 10.00000545]\n",
      " [ 9.99999733  3.9999985   8.99999944 10.00000287]\n",
      " [ 8.00000052  9.00000039 10.00000022  4.99999947]\n",
      " [ 9.00000409  8.00000242 10.00000102  9.99999573]\n",
      " [ 9.99999498  4.99999714  8.9999989   9.00000534]\n",
      " [ 5.99999879  3.99999936  9.9999998   6.00000132]\n",
      " [ 9.00000128  8.0000008  10.00000039  8.99999872]\n",
      " [10.00000374  5.00000222  9.00000094  7.9999961 ]\n",
      " [ 6.99999855  7.99999916  9.99999968  8.00000158]\n",
      " [ 9.00000401  5.0000023   9.00000093  6.99999581]\n",
      " [ 8.99999868  7.99999921  9.99999968  8.00000143]\n",
      " [ 9.00000309 10.00000175 10.0000007   8.99999678]\n",
      " [ 9.99999297  8.99999592  9.99999836  8.00000745]\n",
      " [ 4.9999996   7.99999981  4.99999996  8.00000046]\n",
      " [ 9.99999993  8.         10.00000006 10.00000014]\n",
      " [ 9.00000193  9.00000116 10.00000051  9.99999801]\n",
      " [ 8.99999682  7.99999822  7.99999932  8.00000338]\n",
      " [10.00000162  8.00000097  1.00000044  9.99999835]\n",
      " [ 4.99999909  5.9999995   9.99999983 10.00000099]\n",
      " [ 8.00000542  7.00000316 10.00000129  6.9999943 ]]\n",
      "[[10. 10. 10. 10.]\n",
      " [10.  4.  9. 10.]\n",
      " [ 8.  9. 10.  5.]\n",
      " [ 9.  8. 10. 10.]\n",
      " [10.  5.  9.  9.]\n",
      " [ 6.  4. 10.  6.]\n",
      " [ 9.  8. 10.  9.]\n",
      " [10.  5.  9.  8.]\n",
      " [ 7.  8. 10.  8.]\n",
      " [ 9.  5.  9.  7.]\n",
      " [ 9.  8. 10.  8.]\n",
      " [ 9. 10. 10.  9.]\n",
      " [10.  9. 10.  8.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [10.  8. 10. 10.]\n",
      " [ 9.  9. 10. 10.]\n",
      " [ 9.  8.  8.  8.]\n",
      " [10.  8.  1. 10.]\n",
      " [ 5.  6. 10. 10.]\n",
      " [ 8.  7. 10.  7.]]\n"
     ]
    }
   ],
   "source": [
    "#Compare the predicted and actual results\n",
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your summary here.**\n",
    "\n",
    "The approximation and the original data are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last time we placed an **np.nan** value into this matrix the entire svd algorithm in python broke.  Let's see if that is still the case using your FunkSVD function.  In the below cell, I have placed a nan into the first cell of your numpy array.  \n",
    "\n",
    "`4.` Use 4 latent features, a learning rate of 0.005, and 250 iterations.  Are you able to run your SVD without it breaking (something that was not true about the python built in)?  Do you get a prediction for the nan value?  What is your prediction for the missing value? Use the cells below to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[nan, 10., 10., 10.],\n",
       "        [10.,  4.,  9., 10.],\n",
       "        [ 8.,  9., 10.,  5.],\n",
       "        [ 9.,  8., 10., 10.],\n",
       "        [10.,  5.,  9.,  9.],\n",
       "        [ 6.,  4., 10.,  6.],\n",
       "        [ 9.,  8., 10.,  9.],\n",
       "        [10.,  5.,  9.,  8.],\n",
       "        [ 7.,  8., 10.,  8.],\n",
       "        [ 9.,  5.,  9.,  7.],\n",
       "        [ 9.,  8., 10.,  8.],\n",
       "        [ 9., 10., 10.,  9.],\n",
       "        [10.,  9., 10.,  8.],\n",
       "        [ 5.,  8.,  5.,  8.],\n",
       "        [10.,  8., 10., 10.],\n",
       "        [ 9.,  9., 10., 10.],\n",
       "        [ 9.,  8.,  8.,  8.],\n",
       "        [10.,  8.,  1., 10.],\n",
       "        [ 5.,  6., 10., 10.],\n",
       "        [ 8.,  7., 10.,  7.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are placing a nan into our original subset matrix\n",
    "ratings_mat[0, 0] = np.nan\n",
    "ratings_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Statistics\n",
      "Iterations | Mean Squared Error \n",
      "#1 sse_accum is 3518.594526 and delta is 3518.594526\n",
      "#2 sse_accum is 1251.120644 and delta is -2267.473882\n",
      "#3 sse_accum is 339.895532 and delta is -911.225112\n",
      "#4 sse_accum is 218.976377 and delta is -120.919154\n",
      "#5 sse_accum is 204.375360 and delta is -14.601018\n",
      "#6 sse_accum is 197.881508 and delta is -6.493851\n",
      "#7 sse_accum is 192.200657 and delta is -5.680851\n",
      "#8 sse_accum is 186.266714 and delta is -5.933943\n",
      "#9 sse_accum is 179.771813 and delta is -6.494901\n",
      "#10 sse_accum is 172.593993 and delta is -7.177820\n",
      "#11 sse_accum is 164.683366 and delta is -7.910627\n",
      "#12 sse_accum is 156.039668 and delta is -8.643698\n",
      "#13 sse_accum is 146.711333 and delta is -9.328334\n",
      "#14 sse_accum is 136.797701 and delta is -9.913633\n",
      "#15 sse_accum is 126.447770 and delta is -10.349931\n",
      "#16 sse_accum is 115.852468 and delta is -10.595302\n",
      "#17 sse_accum is 105.229573 and delta is -10.622895\n",
      "#18 sse_accum is 94.802799 and delta is -10.426774\n",
      "#19 sse_accum is 84.778780 and delta is -10.024019\n",
      "#20 sse_accum is 75.326885 and delta is -9.451895\n",
      "#21 sse_accum is 66.566287 and delta is -8.760598\n",
      "#22 sse_accum is 58.562518 and delta is -8.003769\n",
      "#23 sse_accum is 51.332837 and delta is -7.229681\n",
      "#24 sse_accum is 44.857416 and delta is -6.475421\n",
      "#25 sse_accum is 39.092465 and delta is -5.764951\n",
      "#26 sse_accum is 33.982045 and delta is -5.110420\n",
      "#27 sse_accum is 29.466748 and delta is -4.515297\n",
      "#28 sse_accum is 25.488898 and delta is -3.977850\n",
      "#29 sse_accum is 21.994907 and delta is -3.493991\n",
      "#30 sse_accum is 18.935796 and delta is -3.059111\n",
      "#31 sse_accum is 16.266819 and delta is -2.668977\n",
      "#32 sse_accum is 13.946834 and delta is -2.319985\n",
      "#33 sse_accum is 11.937772 and delta is -2.009062\n",
      "#34 sse_accum is 10.204313 and delta is -1.733458\n",
      "#35 sse_accum is 8.713759 and delta is -1.490554\n",
      "#36 sse_accum is 7.436014 and delta is -1.277745\n",
      "#37 sse_accum is 6.343615 and delta is -1.092400\n",
      "#38 sse_accum is 5.411744 and delta is -0.931871\n",
      "#39 sse_accum is 4.618204 and delta is -0.793540\n",
      "#40 sse_accum is 3.943330 and delta is -0.674874\n",
      "#41 sse_accum is 3.369860 and delta is -0.573469\n",
      "#42 sse_accum is 2.882767 and delta is -0.487093\n",
      "#43 sse_accum is 2.469057 and delta is -0.413711\n",
      "#44 sse_accum is 2.117566 and delta is -0.351491\n",
      "#45 sse_accum is 1.818754 and delta is -0.298812\n",
      "#46 sse_accum is 1.564501 and delta is -0.254253\n",
      "#47 sse_accum is 1.347922 and delta is -0.216579\n",
      "#48 sse_accum is 1.163195 and delta is -0.184727\n",
      "#49 sse_accum is 1.005405 and delta is -0.157789\n",
      "#50 sse_accum is 0.870414 and delta is -0.134992\n",
      "#51 sse_accum is 0.754735 and delta is -0.115679\n",
      "#52 sse_accum is 0.655435 and delta is -0.099299\n",
      "#53 sse_accum is 0.570047 and delta is -0.085388\n",
      "#54 sse_accum is 0.496494 and delta is -0.073554\n",
      "#55 sse_accum is 0.433024 and delta is -0.063470\n",
      "#56 sse_accum is 0.378161 and delta is -0.054862\n",
      "#57 sse_accum is 0.330660 and delta is -0.047501\n",
      "#58 sse_accum is 0.289465 and delta is -0.041195\n",
      "#59 sse_accum is 0.253684 and delta is -0.035781\n",
      "#60 sse_accum is 0.222559 and delta is -0.031125\n",
      "#61 sse_accum is 0.195445 and delta is -0.027114\n",
      "#62 sse_accum is 0.171793 and delta is -0.023652\n",
      "#63 sse_accum is 0.151135 and delta is -0.020658\n",
      "#64 sse_accum is 0.133070 and delta is -0.018065\n",
      "#65 sse_accum is 0.117253 and delta is -0.015816\n",
      "#66 sse_accum is 0.103391 and delta is -0.013862\n",
      "#67 sse_accum is 0.091229 and delta is -0.012162\n",
      "#68 sse_accum is 0.080549 and delta is -0.010680\n",
      "#69 sse_accum is 0.071161 and delta is -0.009388\n",
      "#70 sse_accum is 0.062903 and delta is -0.008259\n",
      "#71 sse_accum is 0.055632 and delta is -0.007271\n",
      "#72 sse_accum is 0.049226 and delta is -0.006406\n",
      "#73 sse_accum is 0.043577 and delta is -0.005648\n",
      "#74 sse_accum is 0.038594 and delta is -0.004983\n",
      "#75 sse_accum is 0.034194 and delta is -0.004399\n",
      "#76 sse_accum is 0.030308 and delta is -0.003886\n",
      "#77 sse_accum is 0.026874 and delta is -0.003435\n",
      "#78 sse_accum is 0.023836 and delta is -0.003037\n",
      "#79 sse_accum is 0.021149 and delta is -0.002687\n",
      "#80 sse_accum is 0.018771 and delta is -0.002378\n",
      "#81 sse_accum is 0.016665 and delta is -0.002106\n",
      "#82 sse_accum is 0.014799 and delta is -0.001866\n",
      "#83 sse_accum is 0.013146 and delta is -0.001653\n",
      "#84 sse_accum is 0.011680 and delta is -0.001466\n",
      "#85 sse_accum is 0.010380 and delta is -0.001300\n",
      "#86 sse_accum is 0.009227 and delta is -0.001153\n",
      "#87 sse_accum is 0.008204 and delta is -0.001023\n",
      "#88 sse_accum is 0.007295 and delta is -0.000908\n",
      "#89 sse_accum is 0.006489 and delta is -0.000807\n",
      "#90 sse_accum is 0.005772 and delta is -0.000716\n",
      "#91 sse_accum is 0.005136 and delta is -0.000636\n",
      "#92 sse_accum is 0.004570 and delta is -0.000565\n",
      "#93 sse_accum is 0.004068 and delta is -0.000503\n",
      "#94 sse_accum is 0.003621 and delta is -0.000447\n",
      "#95 sse_accum is 0.003224 and delta is -0.000397\n",
      "#96 sse_accum is 0.002870 and delta is -0.000353\n",
      "#97 sse_accum is 0.002556 and delta is -0.000314\n",
      "#98 sse_accum is 0.002277 and delta is -0.000280\n",
      "#99 sse_accum is 0.002028 and delta is -0.000249\n",
      "#100 sse_accum is 0.001806 and delta is -0.000221\n",
      "#101 sse_accum is 0.001609 and delta is -0.000197\n",
      "#102 sse_accum is 0.001434 and delta is -0.000175\n",
      "#103 sse_accum is 0.001278 and delta is -0.000156\n",
      "#104 sse_accum is 0.001139 and delta is -0.000139\n",
      "#105 sse_accum is 0.001015 and delta is -0.000124\n",
      "#106 sse_accum is 0.000904 and delta is -0.000110\n",
      "#107 sse_accum is 0.000806 and delta is -0.000098\n",
      "#108 sse_accum is 0.000719 and delta is -0.000088\n",
      "#109 sse_accum is 0.000641 and delta is -0.000078\n",
      "#110 sse_accum is 0.000571 and delta is -0.000069\n",
      "#111 sse_accum is 0.000509 and delta is -0.000062\n",
      "#112 sse_accum is 0.000454 and delta is -0.000055\n",
      "#113 sse_accum is 0.000405 and delta is -0.000049\n",
      "#114 sse_accum is 0.000361 and delta is -0.000044\n",
      "#115 sse_accum is 0.000322 and delta is -0.000039\n",
      "#116 sse_accum is 0.000287 and delta is -0.000035\n",
      "#117 sse_accum is 0.000256 and delta is -0.000031\n",
      "#118 sse_accum is 0.000228 and delta is -0.000028\n",
      "#119 sse_accum is 0.000204 and delta is -0.000025\n",
      "#120 sse_accum is 0.000182 and delta is -0.000022\n",
      "#121 sse_accum is 0.000162 and delta is -0.000020\n",
      "#122 sse_accum is 0.000145 and delta is -0.000018\n",
      "#123 sse_accum is 0.000129 and delta is -0.000016\n",
      "#124 sse_accum is 0.000115 and delta is -0.000014\n",
      "#125 sse_accum is 0.000103 and delta is -0.000012\n",
      "#126 sse_accum is 0.000091 and delta is -0.000011\n",
      "#127 sse_accum is 0.000082 and delta is -0.000010\n",
      "#128 sse_accum is 0.000073 and delta is -0.000009\n",
      "#129 sse_accum is 0.000065 and delta is -0.000008\n",
      "#130 sse_accum is 0.000058 and delta is -0.000007\n",
      "#131 sse_accum is 0.000052 and delta is -0.000006\n",
      "#132 sse_accum is 0.000046 and delta is -0.000006\n",
      "#133 sse_accum is 0.000041 and delta is -0.000005\n",
      "#134 sse_accum is 0.000037 and delta is -0.000004\n",
      "#135 sse_accum is 0.000033 and delta is -0.000004\n",
      "#136 sse_accum is 0.000029 and delta is -0.000004\n",
      "#137 sse_accum is 0.000026 and delta is -0.000003\n",
      "#138 sse_accum is 0.000023 and delta is -0.000003\n",
      "#139 sse_accum is 0.000021 and delta is -0.000003\n",
      "#140 sse_accum is 0.000019 and delta is -0.000002\n",
      "#141 sse_accum is 0.000017 and delta is -0.000002\n",
      "#142 sse_accum is 0.000015 and delta is -0.000002\n",
      "#143 sse_accum is 0.000013 and delta is -0.000002\n",
      "#144 sse_accum is 0.000012 and delta is -0.000001\n",
      "#145 sse_accum is 0.000010 and delta is -0.000001\n",
      "#146 sse_accum is 0.000009 and delta is -0.000001\n",
      "#147 sse_accum is 0.000008 and delta is -0.000001\n",
      "#148 sse_accum is 0.000007 and delta is -0.000001\n",
      "#149 sse_accum is 0.000007 and delta is -0.000001\n",
      "#150 sse_accum is 0.000006 and delta is -0.000001\n",
      "#151 sse_accum is 0.000005 and delta is -0.000001\n",
      "#152 sse_accum is 0.000005 and delta is -0.000001\n",
      "#153 sse_accum is 0.000004 and delta is -0.000001\n",
      "#154 sse_accum is 0.000004 and delta is -0.000000\n",
      "#155 sse_accum is 0.000003 and delta is -0.000000\n",
      "#156 sse_accum is 0.000003 and delta is -0.000000\n",
      "#157 sse_accum is 0.000003 and delta is -0.000000\n",
      "#158 sse_accum is 0.000002 and delta is -0.000000\n",
      "#159 sse_accum is 0.000002 and delta is -0.000000\n",
      "#160 sse_accum is 0.000002 and delta is -0.000000\n",
      "#161 sse_accum is 0.000002 and delta is -0.000000\n",
      "#162 sse_accum is 0.000002 and delta is -0.000000\n",
      "#163 sse_accum is 0.000001 and delta is -0.000000\n",
      "#164 sse_accum is 0.000001 and delta is -0.000000\n",
      "#165 sse_accum is 0.000001 and delta is -0.000000\n",
      "#166 sse_accum is 0.000001 and delta is -0.000000\n",
      "#167 sse_accum is 0.000001 and delta is -0.000000\n",
      "#168 sse_accum is 0.000001 and delta is -0.000000\n",
      "#169 sse_accum is 0.000001 and delta is -0.000000\n",
      "#170 sse_accum is 0.000001 and delta is -0.000000\n",
      "#171 sse_accum is 0.000001 and delta is -0.000000\n",
      "#172 sse_accum is 0.000000 and delta is -0.000000\n",
      "#173 sse_accum is 0.000000 and delta is -0.000000\n",
      "#174 sse_accum is 0.000000 and delta is -0.000000\n",
      "#175 sse_accum is 0.000000 and delta is -0.000000\n",
      "#176 sse_accum is 0.000000 and delta is -0.000000\n",
      "#177 sse_accum is 0.000000 and delta is -0.000000\n",
      "#178 sse_accum is 0.000000 and delta is -0.000000\n",
      "#179 sse_accum is 0.000000 and delta is -0.000000\n",
      "#180 sse_accum is 0.000000 and delta is -0.000000\n",
      "#181 sse_accum is 0.000000 and delta is -0.000000\n",
      "#182 sse_accum is 0.000000 and delta is -0.000000\n",
      "#183 sse_accum is 0.000000 and delta is -0.000000\n",
      "#184 sse_accum is 0.000000 and delta is -0.000000\n",
      "#185 sse_accum is 0.000000 and delta is -0.000000\n",
      "#186 sse_accum is 0.000000 and delta is -0.000000\n",
      "#187 sse_accum is 0.000000 and delta is -0.000000\n",
      "#188 sse_accum is 0.000000 and delta is -0.000000\n",
      "#189 sse_accum is 0.000000 and delta is -0.000000\n",
      "#190 sse_accum is 0.000000 and delta is -0.000000\n",
      "#191 sse_accum is 0.000000 and delta is -0.000000\n",
      "#192 sse_accum is 0.000000 and delta is -0.000000\n",
      "#193 sse_accum is 0.000000 and delta is -0.000000\n",
      "#194 sse_accum is 0.000000 and delta is -0.000000\n",
      "#195 sse_accum is 0.000000 and delta is -0.000000\n",
      "#196 sse_accum is 0.000000 and delta is -0.000000\n",
      "#197 sse_accum is 0.000000 and delta is -0.000000\n",
      "#198 sse_accum is 0.000000 and delta is -0.000000\n",
      "#199 sse_accum is 0.000000 and delta is -0.000000\n",
      "#200 sse_accum is 0.000000 and delta is -0.000000\n",
      "#201 sse_accum is 0.000000 and delta is -0.000000\n",
      "#202 sse_accum is 0.000000 and delta is -0.000000\n",
      "#203 sse_accum is 0.000000 and delta is -0.000000\n",
      "#204 sse_accum is 0.000000 and delta is -0.000000\n",
      "#205 sse_accum is 0.000000 and delta is -0.000000\n",
      "#206 sse_accum is 0.000000 and delta is -0.000000\n",
      "#207 sse_accum is 0.000000 and delta is -0.000000\n",
      "#208 sse_accum is 0.000000 and delta is -0.000000\n",
      "#209 sse_accum is 0.000000 and delta is -0.000000\n",
      "#210 sse_accum is 0.000000 and delta is -0.000000\n",
      "#211 sse_accum is 0.000000 and delta is -0.000000\n",
      "#212 sse_accum is 0.000000 and delta is -0.000000\n",
      "#213 sse_accum is 0.000000 and delta is -0.000000\n",
      "#214 sse_accum is 0.000000 and delta is -0.000000\n",
      "#215 sse_accum is 0.000000 and delta is -0.000000\n",
      "#216 sse_accum is 0.000000 and delta is -0.000000\n",
      "#217 sse_accum is 0.000000 and delta is -0.000000\n",
      "#218 sse_accum is 0.000000 and delta is -0.000000\n",
      "#219 sse_accum is 0.000000 and delta is -0.000000\n",
      "#220 sse_accum is 0.000000 and delta is -0.000000\n",
      "#221 sse_accum is 0.000000 and delta is -0.000000\n",
      "#222 sse_accum is 0.000000 and delta is -0.000000\n",
      "#223 sse_accum is 0.000000 and delta is -0.000000\n",
      "#224 sse_accum is 0.000000 and delta is -0.000000\n",
      "#225 sse_accum is 0.000000 and delta is -0.000000\n",
      "#226 sse_accum is 0.000000 and delta is -0.000000\n",
      "#227 sse_accum is 0.000000 and delta is -0.000000\n",
      "#228 sse_accum is 0.000000 and delta is -0.000000\n",
      "#229 sse_accum is 0.000000 and delta is -0.000000\n",
      "#230 sse_accum is 0.000000 and delta is -0.000000\n",
      "#231 sse_accum is 0.000000 and delta is -0.000000\n",
      "#232 sse_accum is 0.000000 and delta is -0.000000\n",
      "#233 sse_accum is 0.000000 and delta is -0.000000\n",
      "#234 sse_accum is 0.000000 and delta is -0.000000\n",
      "#235 sse_accum is 0.000000 and delta is -0.000000\n",
      "#236 sse_accum is 0.000000 and delta is -0.000000\n",
      "#237 sse_accum is 0.000000 and delta is -0.000000\n",
      "#238 sse_accum is 0.000000 and delta is -0.000000\n",
      "#239 sse_accum is 0.000000 and delta is -0.000000\n",
      "#240 sse_accum is 0.000000 and delta is -0.000000\n",
      "#241 sse_accum is 0.000000 and delta is -0.000000\n",
      "#242 sse_accum is 0.000000 and delta is -0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#243 sse_accum is 0.000000 and delta is -0.000000\n",
      "#244 sse_accum is 0.000000 and delta is -0.000000\n",
      "#245 sse_accum is 0.000000 and delta is -0.000000\n",
      "#246 sse_accum is 0.000000 and delta is -0.000000\n",
      "#247 sse_accum is 0.000000 and delta is -0.000000\n",
      "#248 sse_accum is 0.000000 and delta is -0.000000\n",
      "#249 sse_accum is 0.000000 and delta is -0.000000\n",
      "#250 sse_accum is 0.000000 and delta is -0.000000\n"
     ]
    }
   ],
   "source": [
    "# run SVD on the matrix with the missing value\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=250)#use your function with 4 latent features, lr of 0.005 and 250 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the missing rating is 10.278914836516336:\n",
      "\n",
      "The actual value for the missing rating is nan:\n",
      "\n",
      "That's right! You just predicted a rating for a user-movie pair that was never rated!\n",
      "But if you look in the original matrix, this was actually a value of 10. Not bad!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to see if you were able to predict for the missing value\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is {}:\".format(preds[0,0]))\n",
    "print()\n",
    "print(\"The actual value for the missing rating is {}:\".format(ratings_mat[0,0]))\n",
    "print()\n",
    "assert np.isnan(preds[0,0]) == False\n",
    "print(\"That's right! You just predicted a rating for a user-movie pair that was never rated!\")\n",
    "print(\"But if you look in the original matrix, this was actually a value of 10. Not bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend this to a more realistic example. Unfortunately, running this function on your entire user-movie matrix is still not something you likely want to do on your local machine.  However, we can see how well this example extends to 1000 users.  In the above portion, you were using a very small subset of data with no missing values.\n",
    "\n",
    "`5.` Given the size of this matrix, this will take quite a bit of time.  Consider the following hyperparameters: 4 latent features, 0.005 learning rate, and 20 iterations.  Grab a snack, take a walk, and this should be done running in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Statistics\n",
      "Iterations | Mean Squared Error \n",
      "#1 sse_accum is 251896.739393 and delta is 251896.739393\n",
      "#2 sse_accum is 115547.500455 and delta is -136349.238939\n",
      "#3 sse_accum is 79284.992192 and delta is -36262.508262\n",
      "#4 sse_accum is 61242.172611 and delta is -18042.819582\n",
      "#5 sse_accum is 49885.259158 and delta is -11356.913452\n",
      "#6 sse_accum is 41945.153802 and delta is -7940.105357\n",
      "#7 sse_accum is 36040.025299 and delta is -5905.128502\n",
      "#8 sse_accum is 31466.947899 and delta is -4573.077400\n",
      "#9 sse_accum is 27824.544747 and delta is -3642.403152\n",
      "#10 sse_accum is 24864.248622 and delta is -2960.296125\n",
      "#11 sse_accum is 22421.700978 and delta is -2442.547644\n",
      "#12 sse_accum is 20382.118782 and delta is -2039.582197\n",
      "#13 sse_accum is 18661.998051 and delta is -1720.120731\n",
      "#14 sse_accum is 17198.951593 and delta is -1463.046458\n",
      "#15 sse_accum is 15945.531321 and delta is -1253.420272\n",
      "#16 sse_accum is 14865.082468 and delta is -1080.448854\n",
      "#17 sse_accum is 13928.798023 and delta is -936.284444\n",
      "#18 sse_accum is 13113.605166 and delta is -815.192857\n",
      "#19 sse_accum is 12400.674693 and delta is -712.930473\n",
      "#20 sse_accum is 11774.397655 and delta is -626.277038\n"
     ]
    }
   ],
   "source": [
    "# Setting up a matrix of the first 1000 users with movie ratings\n",
    "first_1000_users = np.matrix(user_by_movie.head(1000))\n",
    "\n",
    "# perform funkSVD on the matrix of the top 1000 users\n",
    "user_mat, movie_mat = FunkSVD(first_1000_users, 4, 0.005, 20) #fit to 1000 users with 4 latent features, lr of 0.005, and 20 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Now that you have a set of predictions for each user-movie pair,  let's answer a few questions about your results. Provide the correct values for each of the variables below, and check your solutions using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actual ratings in the first_1000_users is 10852.\n",
      "\n",
      "The number of ratings made for user-movie pairs that didn't have ratings is 31234148\n"
     ]
    }
   ],
   "source": [
    "# Replace each of the comments below with the correct values\n",
    "num_user = first_1000_users.shape[0]\n",
    "num_movie = first_1000_users.shape[1]\n",
    "num_ratings = num_user*num_movie-np.sum(np.isnan(first_1000_users))# How many actual ratings exist in first_1000_users\n",
    "print(\"The number of actual ratings in the first_1000_users is {}.\".format(num_ratings))\n",
    "print()\n",
    "\n",
    "\n",
    "ratings_for_missing = np.sum(np.isnan(first_1000_users)) # How many ratings did we make for user-movie pairs that didn't actually have ratings\n",
    "print(\"The number of ratings made for user-movie pairs that didn't have ratings is {}\".format(ratings_for_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\n"
     ]
    }
   ],
   "source": [
    "# Test your results against the solution\n",
    "assert num_ratings == 10852, \"Oops!  The number of actual ratings doesn't quite look right.\"\n",
    "assert ratings_for_missing == 31234148, \"Oops!  The number of movie-user pairs that you made ratings for that didn't actually have ratings doesn't look right.\"\n",
    "\n",
    "# Make sure you made predictions on all the missing user-movie pairs\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "assert np.isnan(preds).sum() == 0\n",
    "print(\"Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.25535203, 5.11790583, 3.54533404, ..., 5.7805731 , 5.89873826,\n",
       "        3.92631154],\n",
       "       [3.8846001 , 3.87180992, 3.41021228, ..., 6.00371102, 6.09892903,\n",
       "        4.60339827],\n",
       "       [3.13483773, 4.01359303, 3.14207777, ..., 5.12571378, 5.22751267,\n",
       "        3.62920951],\n",
       "       ...,\n",
       "       [3.07051418, 3.33333189, 2.80634088, ..., 4.83107934, 4.95007445,\n",
       "        3.69205198],\n",
       "       [3.86783205, 4.84624249, 3.60221708, ..., 6.48843803, 6.82044688,\n",
       "        5.30911188],\n",
       "       [2.90571153, 3.03412032, 2.69359235, ..., 4.45339072, 4.48558944,\n",
       "        3.18969812]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
